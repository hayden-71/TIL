{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa7b6989",
   "metadata": {},
   "source": [
    "# Memory (ëŒ€í™” ë‚´ìš© ê¸°ì–µ)\n",
    "- LLMì€ ê¸°ë³¸ì ìœ¼ë¡œ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ì§€ ì•ŠìŒ (stateless)\n",
    "- ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê³„ì† í”„ë¡¬í”„íŠ¸ì— ì£¼ì…í•´ì•¼ í•¨\n",
    "\n",
    "1. short-term memory\n",
    "- ë‹¨ê¸°ê¸°ì–µ: í•œ ëŒ€í™” ì„¸ì…˜ì— ëŒ€í•œ ê¸°ì–µ\n",
    "\n",
    "1. long-term memory\n",
    "- ì¥ê¸°ê¸°ì–µ: ì „ì²´ ì„¸ì…˜ì—ì„œ ì¶”ì¶œí•œ ì¤‘ìš” ì •ë³´\n",
    "\n",
    "## Memory êµ¬ë™ ë°©ì‹\n",
    "1. ë©”ëª¨ë¦¬ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë“  ëŒ€í™” ë‚´ì—­ì„ LLM Inputì— ë°€ì–´ë„£ëŠ”ê²ƒ\n",
    "1. ì´ë•Œ ëŒ€í™”ê°€ ê¸¸ì–´ì§€ë©´ í† í° ìˆ˜ ì¦ê°€ ë° ì„±ëŠ¥ í•˜ë½ì´ ì¼ì–´ë‚¨\n",
    "1. ê°œì„  ë°©ì‹\n",
    "    1. ìš”ì•½\n",
    "    1. ì ì • ê¸¸ì´ì—ì„œ ì•ë¶€ë¶„ ìë¥´ê¸°\n",
    "    1. ì •ë¦¬ (íŠ¹ì • ëª…ì‚¬ë“¤ë¡œ ì •ë¦¬, Node-Edge ê·¸ë˜í”„ ë°©ì‹ ë“±..)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd4a24d",
   "metadata": {},
   "source": [
    "## `ConversationBufferMemory`\n",
    "- ë©”ì‹œì§€ ì €ì¥ -> ë³€ìˆ˜ì—ì„œ ì¶”ì¶œ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4e776f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4.1-nano', temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        ('system', 'ë„Œ ìœ ìš©í•œ ì±—ë´‡ì´ì•¼'),\n",
    "        MessagesPlaceholder(variable_name='chat_history'), # ê¸°ì¡´ ì±„íŒ… ë‚´ì—­ì„ ë‹¤ ì£¼ì…\n",
    "        ('human', 'hi')\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key='chat_history')\n",
    "\n",
    "# ë©”ëª¨ë¦¬ë¥¼ ì €ì¥í•  ë³€ìˆ˜ëŠ” {}ë‹¤. ê¸°ì¡´ì— ëŒ€í™”ë‚´ìš©ì´ ìˆë‹¤ë©´ ë¶ˆëŸ¬ì™€ë¼ã…\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68d579cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¸ê°„:  ì•ˆë…•\n",
      "AI:  ì•ˆë…•~ ê·€ì—¬ìš´ ì¸ì‚¬ì•¼! ì˜¤ëŠ˜ë„ í–‰ë³µí•œ í•˜ë£¨ ë³´ë‚´ê³  ìˆì§€? ğŸ˜Š\n",
      "ì¸ê°„:  ì›… ë‚´ ì´ë¦„ì€ í—¤ì´ë“ \n",
      "AI:  í—¤ì´ë“ ! ë°˜ê°€ì›Œ~ ë‚´ ì´ë¦„ë„ ê·€ì—½ê²Œ ë¶ˆëŸ¬ì¤˜ì„œ ê³ ë§ˆì›Œ! ì˜¤ëŠ˜ì€ ì–´ë–¤ ì¼ë“¤ì´ ê¸°ë‹¤ë¦¬ê³  ìˆì„ê¹Œ? ê¶ê¸ˆí•œ ê±° ìˆìœ¼ë©´ ì–¸ì œë“  ë§í•´ì¤˜~ ğŸ˜Š\n",
      "ì¸ê°„:  ë‚´ ì´ë¦„ì´ ë­ê²Œ\n",
      "AI:  í—¤ì´ë“ ì´ì•¼! ë§ì§€? ë‚´ê°€ ê¸°ì–µí•˜ê³  ìˆì§€~ ì–¸ì œë“ ì§€ ë§í•´ì¤˜ì„œ ê³ ë§ˆì›Œ! ë˜ ê¶ê¸ˆí•œ ê±° ìˆìœ¼ë©´ ë§í•´ì¤˜~ ê·€ì—¬ìš´ ì¹œêµ¬ì•¼! ğŸ˜Š\n",
      "ì¸ê°„:  ì›… ê·¸ë§Œ\n",
      "AI:  ì•Œê² ì–´, í—¤ì´ë“ ! ì–¸ì œë“ ì§€ ë§í•˜ê³  ì‹¶ì„ ë•Œ ë§í•´ì¤˜~ ë‚˜ëŠ” ì—¬ê¸°ì„œ ê¸°ë‹¤ë¦¬ê³  ìˆì„ê²Œ! ğŸ˜Š\n",
      "ì¸ê°„:  \n",
      "AI:  í—¤ì´ë“ , í˜¹ì‹œ ë” í•˜ê³  ì‹¶ì€ ë§ì´ ìˆê±°ë‚˜ ê¶ê¸ˆí•œ ê±° ìˆìœ¼ë©´ ì–¸ì œë“  ë§í•´ì¤˜! ë‚˜ëŠ” í•­ìƒ ì—¬ê¸°ì„œ ê¸°ë‹¤ë¦¬ê³  ìˆë‹¨ë‹¤~ ğŸ˜Š\n",
      "ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4.1-nano', temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        ('system', 'ë„Œ ë§íˆ¬ê°€ ê·€ì—¬ìš´ ì±—ë´‡ì´ì•¼'),\n",
    "        # ì´ì „ ëŒ€í™”ë‚´ì—­ ìë¦¬ í‘œì‹œì\n",
    "        MessagesPlaceholder(variable_name='chat_history'),  # ê¸°ì¡´ ì±„íŒ… ë‚´ì—­ì„ ë‹¤ ì£¼ì…\n",
    "        ('human', '{input}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ëŒ€í™”ë‚´ìš© ì €ì¥ì†Œ ë§Œë“¦!\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key='chat_history')\n",
    "\n",
    "# `chat_history`ë³€ìˆ˜ì—, load_memory_var ê²°ê³¼ë¥¼ ì €ì¥í•˜ê³ , \n",
    "runnable = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | # ëŒ€í™” ì „ì²´ ê°€ì ¸ì˜¤ê¸°\n",
    "    itemgetter('chat_history') # chat_history í‚¤ ì¶”ì¶œ\n",
    ")\n",
    "\n",
    "chain = runnable | prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "\n",
    "# ì•„ë˜ì— ì‹¤ì œ gptë‘ ëŒ€í™”í•˜ëŠ” ëª¨ìŠµìœ¼ë¡œ ë§Œë“¤ê¸°\n",
    "input_msg = ''\n",
    "\n",
    "# ì‚¬ìš©ìê°€ ('quit', 'ì •ì§€', 'ê·¸ë§Œ') ì¤‘ì— í•˜ë‚˜ë¥¼ ì…ë ¥í•˜ë©´ ëŒ€í™” ì¢…ë£Œ\n",
    "while input_msg not in ('quit', 'ì •ì§€', 'ê·¸ë§Œ'):\n",
    "    input_msg = input()\n",
    "\n",
    "    if input_msg in ('quit', 'ì •ì§€', 'ê·¸ë§Œ'):\n",
    "        print(\"ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "        break\n",
    "\n",
    "    # GPT ë‹µë³€ ìƒì„±\n",
    "    response = chain.invoke({\"input\": input_msg})\n",
    "\n",
    "    response_text = str(response)\n",
    "\n",
    "    # âœ… memoryì— í˜„ì¬ ëŒ€í™” ì €ì¥\n",
    "    memory.save_context({\"input\": input_msg}, {\"output\": response_text})\n",
    "\n",
    "    \n",
    "    # ëŒ€í™” ë¡œì§ + ëŒ€í™”ë‚´ì—­ ì•„ë˜ ì¶œë ¥\n",
    "    print('ì¸ê°„: ', input_msg)\n",
    "    print('AI: ', response)\n",
    "\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c84ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¸ê°„:  ì ¤ë¦¬\n",
      "{'content_kr': 'ì ¤ë¦¬ëŠ” ë‹¤ì–‘í•œ ë§›ê³¼ ìƒ‰ìƒìœ¼ë¡œ ë§Œë“¤ì–´ì§„ ì ¤ë¼í‹´ ë˜ëŠ” ê¸°íƒ€ ì ¤í™” ì„±ë¶„ìœ¼ë¡œ êµ¬ì„±ëœ ì‹í’ˆì…ë‹ˆë‹¤. ì£¼ë¡œ ê³¼ì¼ ë§›ê³¼ '\n",
      "               'ê²°í•©ë˜ì–´ ê°„ì‹ì´ë‚˜ ë””ì €íŠ¸ë¡œ ì¸ê¸°ê°€ ë†’ìœ¼ë©°, ì–´ë¦°ì´ì™€ ì„±ì¸ ëª¨ë‘ì—ê²Œ ì‚¬ë‘ë°›ëŠ” ê°„ì‹ì…ë‹ˆë‹¤. ì ¤ë¦¬ì˜ ì œì¡° ê³¼ì •ì€ '\n",
      "               'ì ¤ë¼í‹´, ì„¤íƒ•, ë¬¼, í–¥ë£Œ ë“±ì„ í˜¼í•©í•˜ì—¬ ê°€ì—´ í›„ ëƒ‰ê°ì‹œí‚¤ëŠ” ë°©ì‹ìœ¼ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. ìµœê·¼ì—ëŠ” ê±´ê°•ì„ ê³ ë ¤í•œ ì €ë‹¹ '\n",
      "               'ë˜ëŠ” ë¹„ê±´ ì ¤ë¦¬ë„ ê°œë°œë˜ê³  ìˆìœ¼ë©°, ë‹¤ì–‘í•œ í˜•íƒœì™€ í¬ê¸°ë¡œ ì‹œì¥ì—ì„œ íŒë§¤ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì ¤ë¦¬ëŠ” ê·¸ íŠ¹ìœ ì˜ ì«„ê¹ƒí•œ '\n",
      "               'ì‹ê°ê³¼ ë‹¬ì½¤í•œ ë§›ìœ¼ë¡œ ì†Œë¹„ìì—ê²Œ ì¦ê±°ì›€ì„ ì œê³µí•˜ë©°, ë‹¤ì–‘í•œ ë§ˆì¼€íŒ… ì „ëµê³¼ ì œí’ˆ ê°œë°œì´ í™œë°œíˆ ì´ë£¨ì–´ì§€ê³  '\n",
      "               'ìˆìŠµë‹ˆë‹¤.',\n",
      " 'content_sp': 'Las gelatinas son productos alimenticios hechos con '\n",
      "               'ingredientes como gelatina, que tienen diferentes sabores y '\n",
      "               'colores. Son populares como snacks o postres, disfrutados '\n",
      "               'tanto por niÃ±os como por adultos. El proceso de fabricaciÃ³n '\n",
      "               'implica mezclar gelatina, azÃºcar, agua y aromas, calentar y '\n",
      "               'luego enfriar para solidificar. En los Ãºltimos aÃ±os, se han '\n",
      "               'desarrollado gelatinas con menos azÃºcar o veganas para atender '\n",
      "               'a las tendencias de salud. Se presentan en diversas formas y '\n",
      "               'tamaÃ±os en el mercado, ofreciendo una textura masticable y un '\n",
      "               'sabor dulce que proporciona placer a los consumidores. La '\n",
      "               'innovaciÃ³n en marketing y desarrollo de productos continÃºa '\n",
      "               'impulsando el crecimiento del mercado de gelatinas.',\n",
      " 'summary': 'ì ¤ë¦¬ì˜ íŠ¹ì„±ê³¼ í™œìš©ì— ëŒ€í•œ ê°œìš”ë¥¼ ì œê³µí•˜ëŠ” ë³´ê³ ì„œì…ë‹ˆë‹¤.',\n",
      " 'title': 'ì ¤ë¦¬ ë³´ê³ ì„œ'}\n",
      "ì¸ê°„:  ê·¸ë§Œí˜€\n",
      "{'content_kr': 'ìš”ì²­í•˜ì‹  ëŒ€ë¡œ ë³´ê³ ì„œ ì‘ì„±ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ì¶”ê°€ë¡œ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”.',\n",
      " 'content_sp': 'Se ha terminado la elaboraciÃ³n del informe segÃºn su solicitud. '\n",
      "               'Si necesita algo mÃ¡s, no dude en decÃ­rmelo.',\n",
      " 'summary': 'ìš”ì²­í•˜ì‹  ë³´ê³ ì„œ ì‘ì„±ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.',\n",
      " 'title': 'ë³´ê³ ì„œ ì‘ì„± ì¢…ë£Œ ì•Œë¦¼'}\n"
     ]
    }
   ],
   "source": [
    "# ë ˆí¬íŠ¸ ì‘ì„± ì±—ë´‡\n",
    "from operator import itemgetter\n",
    "from pprint import pprint\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model='gpt-4.1-nano', temperature=0)\n",
    "\n",
    "# Output parser ì •ì˜\n",
    "class Report(BaseModel):\n",
    "    title: str = Field(..., description='ë³´ê³ ì„œì˜ ì œëª©')\n",
    "    summary: str = Field(..., description='ë³´ê³ ì„œ ìš”ì•½ë³¸')\n",
    "    content_kr: str = Field(..., description='í•œêµ­ì–´ë¡œ ì‘ì„±ëœ ë³´ê³ ì„œì˜ ë‚´ìš©(1000ì ì´ë‚´)')\n",
    "    content_sp: str = Field(..., description='ìŠ¤í˜ì¸ì–´ë¡œ ì‘ì„±ëœ ë³´ê³ ì„œì˜ ë‚´ìš©(1000ì ì´ë‚´)')\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Report)\n",
    "\n",
    "# Promptì— format instructions ì¶”ê°€\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        ('system', 'ë„Œ ë³´ê³ ì„œ ì‘ì„±ì— íŠ¹í™”ëœ ì±—ë´‡ì´ì•¼. '\n",
    "                   'ë°˜ë“œì‹œ ì§€ì •ëœ í˜•ì‹ì— ë§ì¶° JSONìœ¼ë¡œ ë‹µë³€í•´.\\ncontent_krì€ í•œêµ­ì–´, content_spëŠ” ìŠ¤í˜ì¸ì–´ë¡œ ì‘ì„±í•´.\\nFORMAT INSTRUCTION: {format_instructions}'),\n",
    "        MessagesPlaceholder(variable_name='chat_history'),\n",
    "        ('human', '{input}'),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "# Memory\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key='chat_history')\n",
    "\n",
    "runnable = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) |\n",
    "    itemgetter('chat_history')\n",
    ")\n",
    "\n",
    "# ì²´ì¸ êµ¬ì„±\n",
    "chain = runnable | prompt | llm | parser\n",
    "\n",
    "input_msg = ''\n",
    "\n",
    "# ì‚¬ìš©ìê°€ ('quit', 'ì •ì§€', 'ê·¸ë§Œ', '') ì¤‘ì— í•˜ë‚˜ë¥¼ ì…ë ¥í•˜ë©´ ëŒ€í™” ì¢…ë£Œ\n",
    "while 1:\n",
    "    input_msg = input()\n",
    "    if input_msg  in ('quit', 'ì •ì§€', 'ê·¸ë§Œ', ''):\n",
    "        break\n",
    "\n",
    "    print('ì¸ê°„: ', input_msg)\n",
    "    output_msg = chain.invoke({'input': input_msg})\n",
    "    pprint(output_msg.model_dump())\n",
    "    memory.save_context(\n",
    "        {'human': input_msg},\n",
    "        {'ai': output_msg.model_dump_json()}\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
